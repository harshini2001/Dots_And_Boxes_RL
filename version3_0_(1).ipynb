{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    },
    "colab": {
      "name": "version3.0 (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshini2001/Dots_And_Boxes_RL/blob/main/version3_0_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "derived-governor",
        "outputId": "23a1480c-2c0d-4bf4-9cec-02c680474787"
      },
      "source": [
        "!pip install bitstring"
      ],
      "id": "derived-governor",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bitstring\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/fc/ffac2c199d2efe1ec5111f55efeb78f5f2972456df6939fea849f103f9f5/bitstring-3.1.7.tar.gz (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 21.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 25.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 28.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 29.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 20.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 20.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 143kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 153kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 18.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 18.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: bitstring\n",
            "  Building wheel for bitstring (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bitstring: filename=bitstring-3.1.7-cp37-none-any.whl size=37949 sha256=b90b72ed261be672299c8538d917eb65ccdddf66a5e4ffc095776a545f23c80d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/27/f0/8373e26b7de57db03dc18aaaebdd8c26a99da882416f762979\n",
            "Successfully built bitstring\n",
            "Installing collected packages: bitstring\n",
            "Successfully installed bitstring-3.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__W2Y3dLV-tU"
      },
      "source": [
        "## ML ASSIGNMENT BATCH 8\n",
        "- Deepthishree GS\n",
        "- Harshini S\n",
        "- Iswarya GP\n",
        "- Janani R\n",
        "- Swetha M\n",
        "\n",
        "### ***TOPIC*** - REINFORCEMENT LEARNING\n",
        "### ***Problem Taken*** : Playing Dots and Boxes using a Q learning Agent\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/43994542/112790879-f9220480-907d-11eb-8dae-c041684a9523.png)\n",
        "\n"
      ],
      "id": "__W2Y3dLV-tU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVsBio1LWhYt"
      },
      "source": [
        "## Importing Libraries "
      ],
      "id": "aVsBio1LWhYt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "general-white"
      },
      "source": [
        "from bitstring import BitArray\n",
        "# This library is used to store and convert the board state. \n",
        "# Board state is stored as bitarray with one indicating a line and zero indicating no line\n",
        "import numpy as np\n",
        "import random\n",
        "# To make random moves in initial exploration phase\n",
        "import json\n",
        "# Save the QTABLE finally as a json file to continue training incrementally when the kernal restarts"
      ],
      "id": "general-white",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nP-6_l8jWkyw"
      },
      "source": [
        "## Global Variables declaration\n"
      ],
      "id": "nP-6_l8jWkyw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adjusted-moisture"
      },
      "source": [
        "'''\n",
        "3x3 dots and boxes \n",
        "4 boxes can be drawn\n",
        "12 lines to choose from for each move\n",
        "2 power 12 game states for the agent to learn from\n",
        "DRAW / TIE is available in this version of the game (in original version in case of tie, person who starts loses)\n",
        "Reward 100 and Penalty -100 (for each box drawn)\n",
        "Reward 200 and Penalty -200 (long term finally at the end)\n",
        "Reward 50 for a tie game at end\n",
        "'''\n",
        "QTable=dict()\n",
        "# This table is the ultimate QTable to be learnt by our agent\n",
        "'''\n",
        "Q TABLE usually has states as rows and actions as columns and the entries are the corresponding Q values\n",
        "\n",
        "Since in a game, only legal moves are the actions, the number of columns(moves) for each state is different\n",
        "\n",
        "So, a hash table is constructed with keys as states and values as action:qvalue pairs\n",
        "'''\n",
        "VISIT_TABLE=dict()\n",
        "# This table has the count of visits for each state. This ensures that we can check if all states are visited often (An assumption for Q Learning Convergence in TOM MITCHELL book)\n",
        "DISCOUNT_RATE=0.8\n",
        "# This decreases the future rewards by a factor each step\n",
        "LEARNING_RATE=0.2\n",
        "# This makes sure table is learnt at a specified. A balance between existing value and updated value\n",
        "# Small LRate values ensure the table is not drastically updated\n"
      ],
      "id": "adjusted-moisture",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zUSZrQ4Ws-4"
      },
      "source": [
        "### Main Function for LEARNER\n",
        "- Algorithm is play games repeatedly and update the Q Table \n",
        "- Calls subfunction playGame\n",
        "\n",
        "#### Main Functions:\n",
        "- Decide the choice of move for agent as random, simple or qlearner \n",
        "- Ensures where the learner is exploring or exploiting\n",
        "- Random and simple for exploration\n",
        "  - Random makes random moves\n",
        "  - Simple makes the move with least visit count for the next state (Thus makes sure the same state is not repeated often and unvisited states are explored)\n",
        "- Qlearner exploits\n",
        "  - This makes the move where the Q value for the next state is maximum\n",
        "  "
      ],
      "id": "2zUSZrQ4Ws-4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developing-simon"
      },
      "source": [
        "def learner(gameCount=1000): #default no of games if the trainer doesn't specify the games\n",
        "\n",
        "    count=1\n",
        "    # counter to keep track of number of games\n",
        "    choiceOfMove=['random','simple','qlearner']\n",
        "    # choice for deciding exploration or exploitation\n",
        "    while count<gameCount:\n",
        "        print(\"GAME \",count)\n",
        "        #print the game number to know the progress of learner\n",
        "        if count<gameCount/10:\n",
        "            playGame(choiceOfMove[0],0,False)\n",
        "        #First 10% games are random\n",
        "        elif count<gameCount/2:\n",
        "            playGame(choiceOfMove[1],0,False)\n",
        "        #Till 50% the games are moved by a simple agent - Exploring the unvisited states\n",
        "        else:\n",
        "            playGame(choiceOfMove[2],0,False)\n",
        "        #Last 50% of games is exploited by QLearner\n",
        "        count+=1\n",
        "        with open('Qtable.json','w') as jsonfile:\n",
        "          json.dump(QTable,jsonfile)\n",
        "        # Each game, the current QTABLE is backed up incase the kernel restarts and values are lost\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "        \n",
        "        "
      ],
      "id": "developing-simon",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnDmo9LBXAP0"
      },
      "source": [
        "### Initialises the Main Q Table\n",
        "- Q TABLE usually has states as rows and actions as columns and the entries are the corresponding Q values\n",
        "\n",
        "- Since in a game, only legal moves are the actions, the number of columns(moves) for each state is different\n",
        "\n",
        "- So, a hash table is constructed with keys as states and values as action:qvalue pairs\n",
        "\n",
        "- All values are init to 0 now\n"
      ],
      "id": "DnDmo9LBXAP0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "given-storage"
      },
      "source": [
        "\n",
        "def initialiseQTable():\n",
        "    global QTable\n",
        "    global VISIT_TABLE\n",
        "    # The global variables are initialised in this function\n",
        "    for i in range(4096):\n",
        "      '''\n",
        "      The number of states is 2 power 12 which is 4096\n",
        "      QTABLE rows (in our case the keys of the hashtable are 4096 in number )\n",
        "\n",
        "      For each state, each line which can be drawn is a possible action. \n",
        "      Initial state = 000000000000 => has 12 possible actions\n",
        "      Some state    = 000011110000 => has 8 zeros and 4 ones. \n",
        "                      Out of the 12 lines, 4 are drawn. 8 lines are free to be drawn for next move.\n",
        "                      The possible line numbers which can be drawn are the possible actions\n",
        "                      EACH ZERO is a possible action. Here, 1,2,3,4 and 9,10,11,12 are the 8 possible actions. \n",
        "      So, the actions are initialised as another dictionary and 0 is the current initialisation for Q value\n",
        "      '''\n",
        "      currentState=BitArray(uint=i, length=12)\n",
        "      actionQvalues=dict()\n",
        "      # find possible actions\n",
        "      bitString=currentState.bin\n",
        "      # convert bitarray to bitstring to store as key\n",
        "      for index in range(len(bitString)):\n",
        "          if bitString[index]=='0':\n",
        "              actionQvalues[index+1]=0\n",
        "            \n",
        "          #EACH ZERO in this state is a possible action when making next move from this state\n",
        "          # index + 1 because line number index starts from 1 for ease of human player\n",
        "\n",
        "      QTable[currentState.bin]=actionQvalues\n",
        "      VISIT_TABLE[currentState.bin]=0\n",
        "      # EACH State visit count is init to 0 \n",
        "      QTable['111111111111']={0:0}\n",
        "      # FINAL BOARD STATE HAS ONLY ONE POSSIBLE ACTION - NO MOVE - 0 is set as action which is not a valid line number (line numbers start from 1 to 12)\n",
        "\n",
        "\n",
        "                \n",
        "        \n",
        "    "
      ],
      "id": "given-storage",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utaUG0o_XbRN"
      },
      "source": [
        "### Best action given a state\n",
        "Function used by QLearner to select the action or move which has maximum QValue for all possible actioins in the given state"
      ],
      "id": "utaUG0o_XbRN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imported-galaxy"
      },
      "source": [
        "#This is the function to choice best action for the  given state using Qvalue in QTable\n",
        "def bestActionForState(state):\n",
        "    maxQval=-2000 #negative inf\n",
        "    bestAction=0\n",
        "    for action,qval in QTable[state].items():\n",
        "        # QTABLE has all valid actions for current state and their QValues\n",
        "        if qval>maxQval:\n",
        "            maxQval=qval # Find the maximum qvalue\n",
        "            bestAction=action # Find the corresponding best action\n",
        "    return bestAction\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "id": "imported-galaxy",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRkexALmXk0j"
      },
      "source": [
        "### UPDATE QTABLE\n",
        "- This function uses the formula for QLearning and updates the moves in reverse chronological order\n",
        "- This is called after each game is over\n",
        "- This finetuning is what makes the agent learn the game\n",
        "- According to Tom Mitchell book, update for QTable values is as \n",
        "  `Q(s,a) = (1-LR)*Q(s,a)+LR*(rwd+DR*MAX(Q(ns,all a's)))`\n",
        "  - First term is existing term\n",
        "  - Second term is update term\n",
        "  - LR and DR are learning and discount rate respectively"
      ],
      "id": "dRkexALmXk0j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSXBeHnJXlbT"
      },
      "source": [
        "\n",
        "def updateQTable(memoryOfPlayerMoves):\n",
        "    global QTable\n",
        "    memoryOfPlayerMoves=memoryOfPlayerMoves[::-1] #reverse chronological order\n",
        "    '''\n",
        "    The first few moves have no rewards. According to Tom Mitchell book, the sequence of updates have no impact. \n",
        "    Therefore, it is efficient to propagate the effect of rewards from current move to the previous moves. \n",
        "    This helps us indirectly since we have rewards only for end result and certain important points in game.\n",
        "    For remaining moves the immediate reward is 0\n",
        "    '''\n",
        "    for state,action,nextState,reward in memoryOfPlayerMoves:\n",
        "        maxQValForNextState=QTable[nextState][bestActionForState(nextState)]\n",
        "        # Finding the maximum QValue for next state to update future rewards along with a discount\n",
        "        existingValue=(1-LEARNING_RATE)*QTable[state][action]\n",
        "        # Existing value is taken by a fraction as indicated by learning rate to prevent drastic updates in Qtable\n",
        "        updateValue=LEARNING_RATE*(reward+DISCOUNT_RATE*maxQValForNextState)\n",
        "        # UPDATE value is immediate reward + Discount rate * future rewards\n",
        "        # ACCORDING TO an equation in Tom Mitchell book\n",
        "        QTable[state][action]=existingValue+updateValue\n",
        "        VISIT_TABLE[state]+=1\n",
        "        #Since this state is visited, its visit count is incremented\n",
        "        "
      ],
      "id": "gSXBeHnJXlbT",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHHnG5NxX5oa"
      },
      "source": [
        "### Print board state\n",
        "- Print the board state so that the human can understand what are the boxes formed and what could be the potential next move during a game"
      ],
      "id": "PHHnG5NxX5oa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "included-natural"
      },
      "source": [
        "#Printing 3*3 table\n",
        "def printTable(currentState):\n",
        "    count=1\n",
        "    m=7\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            print(\".\\t\", end=\" \")\n",
        "            if j<2:\n",
        "                if currentState[count-1]=='0':\n",
        "                    print(str(count)+\"\\t\",end=\" \") \n",
        "                else:\n",
        "                    print(\"--\\t\",end=\" \")\n",
        "                count+=1\n",
        "                \n",
        "        print(\"\\n\")\n",
        "        if i!=2:\n",
        "            for k in range(3):\n",
        "                if currentState[m-1]=='0':\n",
        "                    print(str(m)+\"\\t\\t\",end=\" \")\n",
        "                else:\n",
        "                    print(\"|\\t\\t\",end=\" \")\n",
        "                m+=1\n",
        "        print(\"\\n\")\n",
        "# printTable('111111111111')"
      ],
      "id": "included-natural",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ukDAdv0YE-p"
      },
      "source": [
        "### MakeMove or ChooseAction\n",
        "- First few games, learner uses random moves\n",
        "- Next it uses an approach to select states with very less visit count. \n",
        "- These are for exploration\n",
        "- After that, the game is carried forward by the QLearner\n",
        "\n",
        "- Human can choose any of the three for choice of making move for his/her opponent"
      ],
      "id": "5ukDAdv0YE-p"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "assigned-resolution"
      },
      "source": [
        "'''\n",
        "  Three choices are considered for a move (ie) Random, simple and qlearner\n",
        "  Random move will choose random move from the actions that are available for a particular board state.\n",
        "  Simple move will choose the action for the state with minimum count.\n",
        "  Qlearner will choose best action for a state using Qvalues in the QTable\n",
        "  Random and Simple move are used for explore new states and actions.\n",
        "  QLearner is for exploitation.\n",
        "'''\n",
        "def MakeMove(currentState,choice):\n",
        "    # possible actions\n",
        "    possibleActions=list(QTable[currentState].keys())\n",
        "    #choiceOfMove=['random','simple','qlearner']\n",
        "    if choice=='random':\n",
        "        #returning random action from the possible actions.\n",
        "        return possibleActions[random.randint(0,len(possibleActions)-1)]\n",
        "        \n",
        "    if choice=='simple':\n",
        "        minCount=1000000\n",
        "        action=0\n",
        "        for action in possibleActions:\n",
        "            #VISIT_TABLE stores how many times the states have been visited for each action.\n",
        "            if VISIT_TABLE[transition(currentState,action)]<minCount:\n",
        "                minCount=VISIT_TABLE[transition(currentState,action)]\n",
        "                bestAction=action     \n",
        "        return bestAction\n",
        "    if choice=='qlearner':\n",
        "        return bestActionForState(currentState)  \n",
        "\n",
        "    \n",
        " "
      ],
      "id": "assigned-resolution",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFxupOr3Yq0R"
      },
      "source": [
        "### Checks if the game is over"
      ],
      "id": "WFxupOr3Yq0R"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "curious-surgery"
      },
      "source": [
        "'''\n",
        "End state for a board is '111111111111' -> all lines are drawn.\n",
        "'''\n",
        "def isBoardFinished(boardState):\n",
        "    return boardState.bin==12*'1'"
      ],
      "id": "curious-surgery",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REPVzlDpYtr3"
      },
      "source": [
        "### Transition function\n",
        "- Takes input as current state and move and gives the next state as output"
      ],
      "id": "REPVzlDpYtr3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "speaking-vocabulary"
      },
      "source": [
        "'''\n",
        "It outputs the next state.\n",
        "Ex: currentState is '100000000000' and current move is line number 12, then\n",
        "it will return '100000000001' as the nextstate\n",
        "'''\n",
        "def transition(currentState,currentMove):\n",
        "    temp=currentState\n",
        "    temp=temp[:currentMove-1]+'1'+temp[currentMove:]\n",
        "    return temp\n"
      ],
      "id": "speaking-vocabulary",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIzvm3FNY0oj"
      },
      "source": [
        "### Counts the number of boxes formed during the move to update points"
      ],
      "id": "UIzvm3FNY0oj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "magnetic-vampire"
      },
      "source": [
        "'''\n",
        "This function will check whether the box is formed or not. If so, it will return the\n",
        "number of boxes formed for that move. If not, it will return 0 .\n",
        "  .    1   .    2     . \n",
        "  \n",
        "  7        8          9\n",
        "\n",
        "  .    3   .     4    .\n",
        "\n",
        "  10       11         12\n",
        "\n",
        "  .    5    .   6      .\n",
        "\n",
        "  Line numbers 1,3,7,8 will form one box and 2,4,8,9 will form another box\n",
        "  and 3,5,10,11 will form third box and 4,6,11,12 will form the last box\n",
        "'''\n",
        "def numberOfNewBoxFormed(box,newState):\n",
        "    count=0\n",
        "    boxBounds=[[1,3,7,8],[2,4,8,9],[3,5,10,11],[4,6,11,12]]\n",
        "    for i in range(len(boxBounds)):\n",
        "      if box[i]==0:\n",
        "        flag=True\n",
        "        for lineNumber in boxBounds[i]:\n",
        "          if newState[lineNumber-1]!='1':\n",
        "            flag=False\n",
        "            break\n",
        "        if flag==True:\n",
        "          count+=1\n",
        "          box[i]=1\n",
        "    return count\n",
        "        \n",
        "                "
      ],
      "id": "magnetic-vampire",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DytSPGSvZS87"
      },
      "source": [
        "### Main function where the game is played\n",
        "\n",
        "Algo\n",
        "- init board state\n",
        "- init players as p1,p2 or p1,human\n",
        "- init current player = p1\n",
        "- init box [0 0 0 0]\n",
        "- while not final state: \n",
        "    - if cp != 'human' : \n",
        "        currentplayer.make move // simple, q learner , random .. 3 boxes..\n",
        "    - else:\n",
        "        accept input\n",
        "    - update currentplayer.memory\n",
        "    - Check if new box formed, update player score\n",
        "    - else \n",
        "    -   toggle the currentplayer\n",
        "    - update board state with current move\n",
        "- update QTABLE with rewards and penalties\n"
      ],
      "id": "DytSPGSvZS87"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stuffed-motivation"
      },
      "source": [
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "def playGame(choiceOfMove,choiceOfPlayer,withHuman=False):\n",
        "    #Game init\n",
        "    player=[\"p1\",\"p2\"]\n",
        "    if withHuman:\n",
        "        player[1]='human'\n",
        "    if choiceOfPlayer==2:\n",
        "      currentPlayer='human'\n",
        "    else:\n",
        "      currentPlayer='p1'\n",
        "    box=[0,0,0,0]\n",
        "    currentState=BitArray(uint=0, length=12)\n",
        "    memory={player[0]:[],player[1]:[]}\n",
        "    score={player[0]:0,player[1]:0}\n",
        "    while isBoardFinished(currentState)==False:\n",
        "        if(currentPlayer!=\"human\"):\n",
        "            currentMove=MakeMove(currentState.bin,choiceOfMove)\n",
        "        else:\n",
        "          possibleActions=list(QTable[currentState.bin].keys())\n",
        "          currentMove=int(input(\"Enter the line number:\"))\n",
        "          while currentMove not in possibleActions:\n",
        "            currentMove=int(input(\"INVALID!Enter the line number:\"))\n",
        "\n",
        "        newState=transition(currentState.bin,currentMove) \n",
        "        memory[currentPlayer].append([currentState.bin,currentMove,newState,0])\n",
        "        numberOfBoxes=numberOfNewBoxFormed(box,newState)\n",
        "        if numberOfBoxes>0:#Short term rewards\n",
        "            score[currentPlayer]+=numberOfBoxes*2\n",
        "            memory[currentPlayer][-1][-1]=100\n",
        "            memory[player[1-player.index(currentPlayer)]][-1][-1]=-100\n",
        "\n",
        "        else:\n",
        "            currentPlayer=player[1-player.index(currentPlayer)]\n",
        "\n",
        "        currentState=BitArray(uint=int(newState,2),length=12)\n",
        "        if player[1]=='human':            \n",
        "            printTable(currentState.bin)\n",
        "            print(\"Score of \",player[0],\" is \",score[player[0]])\n",
        "            print(\"Score of \",player[1],\" is \",score[player[1]])\n",
        "            if isBoardFinished(currentState)==False:\n",
        "              print(\"Next turn for \",currentPlayer)\n",
        "\n",
        "    #find winner \n",
        "    result=\"\"\n",
        "    if score[player[0]]==score[player[1]]:\n",
        "        result='TIE'\n",
        "        print(result)\n",
        "    else:\n",
        "        result=max(score,key=score.get)\n",
        "        print(\"Winner is \",result)\n",
        "    #print(memory[player[0]])\n",
        "    if result=='TIE':\n",
        "        memory[player[0]][-1][-1]=50\n",
        "        memory[player[1]][-1][-1]=50\n",
        "    else: \n",
        "        memory[result][-1][-1]=200\n",
        "        memory[player[1-player.index(result)]][-1][-1]=-200\n",
        "    # print(memory[player[0]])\n",
        "    # print(memory[player[1]])    \n",
        "    #put reward and penalties in last row of the memory table\n",
        "    updateQTable(memory[player[0]])\n",
        "    updateQTable(memory[player[1]])\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "stuffed-motivation",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbTkBOgCZgTc"
      },
      "source": [
        "### Learn and test performance"
      ],
      "id": "VbTkBOgCZgTc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recent-photograph"
      },
      "source": [
        "# initialiseQTable()\n",
        "# learner(gameCount=10000)\n",
        "print(\"Enter 1 -> Player 1 : Computer and Player 2: Human\")\n",
        "print(\"Enter 2 -> Player 1 : Human and Player 2: Computer\")\n",
        "choiceOfPlayer=int(i12nput())\n",
        "playGame(choiceOfMove='qlearner',choiceOfPlayer=choiceOfPlayer,withHuman=True)"
      ],
      "id": "recent-photograph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzg-8JyzZj41"
      },
      "source": [
        "### Save the QTABLE and print visit table\n"
      ],
      "id": "Jzg-8JyzZj41"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmk43wbNmbtq"
      },
      "source": [
        "playGame(choiceOfMove='qlearner',withHuman=True)\n",
        "\n",
        "with open('Qtable.json','w') as jsonfile:\n",
        "  json.dump(QTable,jsonfile)\n",
        "\n",
        "print(VISIT_TABLE)"
      ],
      "id": "xmk43wbNmbtq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl9b3ob4XxP4"
      },
      "source": [
        "## Check the performance of our machine by letting it play N games with random agent and find its winrate\n",
        "\n"
      ],
      "id": "tl9b3ob4XxP4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdzryyk0bUqX"
      },
      "source": [
        "def playGame_AgentVsRandomAgent():\n",
        "\n",
        "    player=[\"p1\",\"p2\"]    \n",
        "    currentPlayer=\"p1\"\n",
        "    box=[0,0,0,0]\n",
        "    currentState=BitArray(uint=0, length=12)\n",
        "    memory={player[0]:[],player[1]:[]}\n",
        "    score={player[0]:0,player[1]:0}\n",
        "    while isBoardFinished(currentState)==False:\n",
        "        if(currentPlayer==\"p1\"):\n",
        "          currentMove=MakeMove(currentState.bin,'qlearner')\n",
        "        else:\n",
        "          currentMove=MakeMove(currentState.bin,'random')\n",
        "          \n",
        "\n",
        "        newState=transition(currentState.bin,currentMove) \n",
        "        memory[currentPlayer].append([currentState.bin,currentMove,newState,0])\n",
        "        numberOfBoxes=numberOfNewBoxFormed(box,newState)\n",
        "        if numberOfBoxes>0:#Short term rewards\n",
        "            score[currentPlayer]+=numberOfBoxes*2\n",
        "            memory[currentPlayer][-1][-1]=100\n",
        "            memory[player[1-player.index(currentPlayer)]][-1][-1]=-100\n",
        "\n",
        "        else:\n",
        "            currentPlayer=player[1-player.index(currentPlayer)]\n",
        "\n",
        "        currentState=BitArray(uint=int(newState,2),length=12)\n",
        "        \n",
        "    #find winner \n",
        "    result=\"\"\n",
        "    if score[player[0]]==score[player[1]]:\n",
        "        result='TIE'\n",
        "        print(result)\n",
        "    else:\n",
        "        result=max(score,key=score.get)\n",
        "        print(\"Winner is \",result)\n",
        "    #print(memory[player[0]])\n",
        "    if result=='TIE':\n",
        "        memory[player[0]][-1][-1]=50\n",
        "        memory[player[1]][-1][-1]=50\n",
        "    else: \n",
        "        memory[result][-1][-1]=200\n",
        "        memory[player[1-player.index(result)]][-1][-1]=-200\n",
        "    # print(memory[player[0]])\n",
        "    # print(memory[player[1]])    \n",
        "    #put reward and penalties in last row of the memory table\n",
        "    updateQTable(memory[player[0]])\n",
        "    updateQTable(memory[player[1]])\n",
        "    return result"
      ],
      "id": "wdzryyk0bUqX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4EvL6ucaqVu"
      },
      "source": [
        "winCount=0\n",
        "loseCount=0\n",
        "N=100\n",
        "for i in range(1,N+1):\n",
        "  winner = playGame_AgentVsRandomAgent()\n",
        "  print('Game ',i,\" : Winner is \",winner)\n",
        "  if winner =='p1':\n",
        "    winCount+=1\n",
        "  if winner =='p2':\n",
        "    loseCount+=1\n",
        "\n",
        "\n",
        "print(\"Of the \",N,\" games, our Q Learner  has winrate =\",winCount/N,\" and loserate =\",loseCount/N,\". It has tie rate of  \", (N-winCount-loseCount)/N ) \n",
        "\n"
      ],
      "id": "p4EvL6ucaqVu",
      "execution_count": null,
      "outputs": []
    }
  ]
}